{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uptill now:** We have learned that how we are trying to fing the best estimate $\\hat{f}(x)$ of $f(x)$ And we have only considered cases where $\\hat{f}(x)$ is of degree 1. But what if is *required* that we consider it of more one degree polynomial. In such scenario, the process of defining the trend pattern and predicting for fututre is called **Polynomial Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's say we estimate $\\hat{f}(x)$ as $\\left(\\hat{\\theta_0} + \\hat{\\theta_1} x + \\hat{\\theta}_2 x^2\\right)$. So our **Mean Squared Error** will become:\n",
    "\n",
    "$$ MSE\\ (\\hat{\\theta}_0, \\hat{\\theta}_1, \\hat{\\theta}_2)\\ =\\ \\frac{\\sum^{N_{train}}_{i=1} \\left[y_i - (\\hat{\\theta_0} + \\hat{\\theta_1} x + \\hat{\\theta}_2 x^2) \\right]^2} {N_{train}}$$\n",
    "\n",
    "It is noticeable that we haven't added any new features but engineered new ones by old ones, like $x^2$ in the above example. This is called **feature engineering**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Does polynomial regression have any similarity with linear regresion? Are they both similar?**\n",
    "\n",
    "**Ans:** Consider in above example, $x^2$, a seperate feature, let's say $z$. Now when we plot our data in $y$ against $x$ and $z$ (which is inherently $x^2$). It is will look like as if data is linear. But when plotted just in original dimensions, that are $y$ against $x$, then data is **NOT** linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CAREFUL :**\n",
    "\n",
    "- Feature Engineering may result in engineering of unneccessary features and can then result in **Overfitting**. Solution to that is **Regularisation**.\n",
    "\n",
    "\n",
    "- Feature engineering in linear regression (resulting in polynomial regression) is not like feature engineering in PCA. In PCA, we do **linear feature engineering** as new features are engineered from the *linear combination* of old features but here we do something called **non-linear feature engineering** as we saw in above example, $x^2$, $xp$ where $p$ is some independent feature etc. *Afterall, projecting the data onto new basis(axes just rotated at particular angles) and project in high dimensional space(add dimensions) are two different things*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: How is feature engineering helping us?**\n",
    "\n",
    "**Ans:** We are trying to make the Pearson Correlation Coefficient as maximum as possible between the features and the response variable (and not between a feature and another feature)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

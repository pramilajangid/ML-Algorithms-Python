{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Binary Logistic Regression** using Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Logistic Regression** is a discriminative classifier, because here, instead of first calculating likelihood probability and the calculating posterior probability, we will directly calculate posterior probability by making a PDF for posterior probability. \n",
    "### Let's see how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know, that our posterior probaility, for each example in our data(rows), for one of the class(if our data has only 2 classes) looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{P(class='1' | X)} = \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, for another class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{P(class='0' | X)} = 1 - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose :** our data(considering preprocessed and normalized data) has N rows(examples) and M columns(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** For each example i in all N examples, there will be actual class that it must belong to. That class will be either `0` or `1` (in encoded form)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Let's see how class column will look like: \n",
    "\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "1\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Now what we are going to make PDF of posterior probability is that we are going to combine posterior probability of each class in a **likelihood function**. Let's see how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A.*** For each example i, posterior probaility can be written as :\n",
    "    \n",
    "$$\\boldsymbol{ \\textbf{P(class=}C_i\\textbf{)}} = (\\hat p)^{C_i}\\ (1-\\hat p)^{1 - C_i}$$ \n",
    "\n",
    "where $\\hat p$ is the posterior probability for class `1` and $C_i$ is the class label for a given example $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***B.*** Combining above formula for all examples :\n",
    "\n",
    "$\\textbf{Likelihood probability} $ for $(x_1 = \\text{'0'}\\ \\cap\\ x_2 = \\text{'0'}\\ \\cap\\ \\dots \\cap\\ x_N = \\text{'1'})\\ $ is:\n",
    "\n",
    "\n",
    "$$\\textbf{L}\\ =\\ \\prod^N_{i = 1}\\ (\\hat p)^{C_i}\\ (1- \\hat p)^{1 - C_i}$$\n",
    "\n",
    "\n",
    "$$\\textbf{where,}\\ \\ \\ \\ \\ {\\hat p}\\ =\\ \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}$$ \n",
    "\n",
    "\n",
    "$$\\textbf{and}\\ C_i \\text{is class label for}\\ i^{\\text{th}} example$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***C.*** Taking both side $\\log_e$, our Likelihood Function becomes:\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}}\\ =\\ \\sum_{i=1}^{N}\\ \\left[C_i\\ .\\log_e{\\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}}\\right]\\ \\ +\\  \\left[(1 - C_i)\\ .\\log_e{\\left(1 - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)}\\right]$$\n",
    "\n",
    "$$\\ $$   \n",
    "\n",
    "This is our final **Log Likelihood Function**, which we have to maximize, i.e:\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\underset{\\hat\\theta_0,\\hat\\theta_1, \\hat\\theta_2}{\\textbf{max}}}\\ \\ \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}$$\n",
    "\n",
    "$$\\textbf{=>}\\ \\ \\ \\text{-}\\ \\boldsymbol{\\underset{\\hat\\theta_0,\\hat\\theta_1, \\hat\\theta_2}{\\textbf{min}}}\\ \\ \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}$$\n",
    "\n",
    "$$\\ $$\n",
    "To dissolve the negative sign outside our optimization problem, we take in inside our log likelihood function and the our new likelihood function and optimization problem becomes :\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{- \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}}\\ =\\ - \\ \\sum_{i=1}^{N}\\ \\left[C_i\\ .\\log_e{\\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}}\\right]\\ \\ \\left[(1 - C_i)\\ .\\log_e{1 - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}}\\right]$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\underset{\\hat\\theta_0,\\hat\\theta_1, \\hat\\theta_2}{\\textbf{min}}}\\ \\ - \\ \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Now we are going to solve our minimization problem using **Gradient Descent**:\n",
    "$$\\ $$\n",
    "\n",
    "$$\\theta_{\\textbf{0}, final}\\ =\\ \\theta_{\\textbf{0}, initial} + \\epsilon\\ .\\frac{\\partial}{\\partial\\theta_{\\textbf{0}}}\\left[\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}\\right] \\Bigg|_{\\theta_0 = \\theta_{0,initial}\\\\ \\theta_1 = \\theta_{1,initial}\\\\ \\theta_2 = \\theta_{2, initial}}$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\theta_{\\text{final}}}\\ = \\ \\begin{bmatrix}\n",
    "                                                \\theta_{\\textbf{1},final} \\\\\n",
    "                                                \\theta_{\\textbf{2},final}\n",
    "                                                \\end{bmatrix}\n",
    "                                                \\ =\\ \\begin{bmatrix}\n",
    "                                                \\theta_{\\textbf{1},initial} \\\\\n",
    "                                                \\theta_{\\textbf{2},initial}\n",
    "                                                \\end{bmatrix} \n",
    "                                                \\ +\\ \\epsilon\\ .\\nabla\\log_e{L(\\theta_0, \\theta_1, \\theta_2) \\Bigg|_{\\theta_0 = \\theta_{0,initial}\\\\ \\theta_1 = \\theta_{1,initial}\\\\ \\theta_2 = \\theta_{2, initial}}}\n",
    "                                                $$\n",
    "                                                \n",
    "                                                \n",
    "$$\\textbf{where,}\\ \\ \\ \\boldsymbol{\\epsilon}\\ \\text{is the step-size}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A.*** Calculating $\\boldsymbol{\\frac{\\partial}{\\partial\\theta_{\\textbf{0}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}$ and $\\boldsymbol{\\nabla\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}$ we find their values as:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial}{\\partial\\theta_{\\textbf{0}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}\\ =\\ -\\ \\sum_{i=0}^{N}\\left(C_i - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\nabla\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}\\ = \\ \\begin{bmatrix}\n",
    "                            \\frac{\\partial}{\\partial\\theta_{\\textbf{1}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)} \\\\\n",
    "                            \\frac{\\partial}{\\partial\\theta_{\\textbf{2}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}\n",
    "                            \\end{bmatrix}\\ = \\ \n",
    "                            \\begin{bmatrix}\n",
    "                 -\\ \\sum\\limits_{i=1}^{N}\\ x_1^i\\ .\\left(C_i - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)\\\\\n",
    "                 -\\ \\sum\\limits_{i=1}^{N}\\ x_2^i\\ .\\left(C_i - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)\n",
    "                 \\end{bmatrix}\n",
    "                            $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Code it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ini = pd.read_csv('/home/pramila/Desktop/DataSets/TumorData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_ini.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels=['Unnamed: 32', 'id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'].replace(to_replace=['M', 'B'], value=[1,0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.iloc[:int(0.75*data.shape[0]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = data.iloc[int(0.75*data.shape[0]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = training_data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "421    0\n",
       "422    0\n",
       "423    0\n",
       "424    0\n",
       "425    0\n",
       "Name: diagnosis, Length: 426, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array(C).reshape(C.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data.drop(labels='diagnosis', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean())-X.std()  #Normalizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_sigmoid_output(theta0, theta1):\n",
    "    '''calculates posterior probability, i.e probability represented as sigmoid function\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(theta0, theta1, C):\n",
    "    '''Calculates log likelihood function's value for given thetas\n",
    "    '''\n",
    "    sigmoid_output = the_sigmoid_output(theta0, theta1)\n",
    "    \n",
    "    first_term = np.matmul(C.T, np.log(sigmoid_output))\n",
    "    second_term = np.matmul((1-C).T, np.log(1 - sigmoid_output))\n",
    "        \n",
    "    return first_term + second_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_theta0():\n",
    "    '''Returns derivate with respect to theta0 we are going to need in gradient descent\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 10**(-7)\n",
    "\n",
    "step_size = 10**(-4)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    theta0_final = theta0_initial + (step_size * (derivative0))\n",
    "    \n",
    "    theta_final = theta_initial + (step_size * (gradient_matrix))\n",
    "    \n",
    "    if abs(neg_log_likelihood(theta0_final, theta_final) - neg_log_likelihood(theta0_final, theta_final)) < tolerance :\n",
    "        break\n",
    "    \n",
    "    theta0_initial =  theta0_final\n",
    "    theta_initial = theta_final\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just directly jump to mathematical explaination and we will each of these terms - Overfitting and Underfitting in our way through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to understand these concepts with the help of **Linear Regressoin**. \n",
    "\n",
    "**Our data**: Let's say we have 1 lakh examples with one feature $X$ and one response variable $Y$. Let's say we pull out 80% of the data for training and 20% for testing.\n",
    "\n",
    "So now our number of training examples are 80,000. Now: From those 80,000 examples we sample out 80,000 examples with replacement 100 times. By 'with replacement' we mean, pull out a example -> see what is it -> put it in the bag -> pull out another example, it can be same as previous one -> and so on, do this 80,000 times, for each time. Hence we have a 100 samples of 80,000 examples in each. This way we have 100 copies of training data, where every copy differs from another copy. \n",
    "\n",
    "Our samples are : $S_1, S_2, \\cdots , S_{100} $\n",
    "\n",
    "We train our linear regression on each of these samples. And for each sample, there will be the best estimate of $f(x)$ as: $\\left(\\hat{f}^1(x), \\hat{f}^2(x), \\cdots , \\hat{f}^{100}(x) \\right)$ where $\\hat{f}^i(x)\\ =\\ \\hat{\\theta}^i_0 + \\hat{\\theta}^i_1 X$ for each $i^{th}$ sample.\n",
    "\n",
    "We can say these samples are trained linear models.\n",
    "\n",
    "- Now pull out one example, say $j^{th}$ from testing data find the error between true value of that testing data and all our trained linear models for that $j^{th}$ example:\n",
    "\n",
    "$$\n",
    "\\hat{f}_1(X_j)\\ =\\ \\hat{y}^1_{j}\\\\\n",
    "\\hat{f}_2(X_j)\\ =\\ \\hat{y}^2_{j} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{f}_{100}(X_j)\\ =\\ \\hat{y}^{100}_{j}$$\n",
    "\n",
    "It can be observed that $\\hat{y}_{j}$ has a distribution rather than a single, definite value.\n",
    "\n",
    "So, our squared error is: \n",
    "\n",
    "$$ \\text{squared error}\\ =\\ \\left(y_j - \\hat{y}_j \\right)^2$$\n",
    "\n",
    "$$\\text{We are going add and subtract }E[\\hat{y}_j] \\text{ in our above squared error as:} $$\n",
    "\n",
    "$$\\text{squared error}\\ =\\ \\left(y_j - E[\\hat{y}_j] + E[\\hat{y}_j] - \\hat{y}_j \\right)^2 $$\n",
    "\n",
    "$$\\text{applying } (a+b)^2 = a^2 + b^2 + 2ab \\\\ \\text{here,}\\ \\ \\ \\ a = y_j - E[\\hat{y}_j] \\\\ \\text{and}\\ \\ \\ \\ b = E[\\hat{y}_j] - \\hat{y}_j$$\n",
    "\n",
    "$$\\text{We get:}\\ \\ \\ \\ \\text{S.E.} = \\left(y_j - \\hat{y}_j \\right)^2 = (y_j - E[\\hat{y}_j])^2 + (E[\\hat{y}_j] - \\hat{y}_j )^2+ 2(y_j - E[\\hat{y}_j])(E[\\hat{y}_j] - \\hat{y}_j) $$\n",
    "\n",
    "Our **Mean Squared Error** for all the testing data will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{MSE}_{test} = \\frac{1}{N_{test}} \\sum^{N_{test}}_{i=1} \\left(y_i - \\hat{y}_i \\right)^2 $$\n",
    "\n",
    "$$\\textbf{Our aim:} \\text{ to find such } \\hat{y}_i \\text{ that our } \\textbf{MSE}_{test} \\text{ is minimum}$$\n",
    "\n",
    "$$\\text{It can be written:} \\textbf{ MSE}_{test} = E[(y - \\hat{y}_i)^2] $$\n",
    "\n",
    "$$\\text{And earlier above from S.E. equation, we found what } \\left(y_j - \\hat{y}_j \\right)^2 \\text{ is equal to. Hence putting the value from above:}$$\n",
    "\n",
    "$$\\textbf{MSE} = E[(y_j - E[\\hat{y}_j])^2] + E[(E[\\hat{y}_j] - \\hat{y}_j )^2] + E[2(y_j - E[\\hat{y}_j])(E[\\hat{y}_j] - \\hat{y}_j)] $$\n",
    "\n",
    "$$\\textbf{Solving it further: }$$\n",
    "\n",
    "$$\\textbf{MSE} = E[(y_j - E[\\hat{y}_j])^2] + E[(E[\\hat{y}_j] - \\hat{y}_j )^2] + 2. \\left(E[Y.E[\\hat{y}]] - E[y.\\hat{y}] - E[E[\\hat{y}]^2] + E[\\hat{y}.E[\\hat{y}]] \\right)$$\n",
    "\n",
    "$$\\text{y is constant because it is the true value}$$\n",
    "$$\\text{E[}\\hat{y}\\text{] is constant because it is the mean of all the predictions from 100 models}\\\\ \\textbf{Hence:}$$\n",
    "\n",
    "$$\\textbf{MSE} = E[(y-E[\\hat{y}])^2] + E[(E[\\hat{y}] - \\hat{y})^2]$$\n",
    "\n",
    "$$\\text{Now, we know:}$$\n",
    "\n",
    "$$\\hat{y} = \\{ \\hat{y}^1, \\hat{y}^2, ... , \\hat{y}^{100} \\}$$\n",
    "\n",
    "$$\\text{So,} E[\\hat{y}] = \\text{average of all the predicted values from all the models} $$\n",
    "\n",
    "$$= \\frac{\\hat{y}^1 + \\hat{y}^2 + ... + \\hat{y}^{100}}{100} $$\n",
    "\n",
    "$$= \\frac{ (\\hat{\\theta}^1_0 + \\hat{\\theta}^1 . x) + (\\hat{\\theta}^2_0 + \\hat{\\theta}^2 . x) + ...(\\hat{\\theta}^{100}_0 + \\hat{\\theta}^{100} . x)}{100}$$\n",
    "\n",
    "$$ = \\frac{\\hat{\\theta}^1_0 + \\hat{\\theta}^2_0 + ... + \\hat{\\theta}^{100}_0}{100} + x. \\frac{\\hat{\\theta}^1 + \\hat{\\theta}^2 + ... + \\hat{\\theta}^{100}}{100}$$\n",
    "\n",
    "$$ \\\\ $$\n",
    "\n",
    "$$ \\text{Hence: }\\ \\ \\ E[\\hat{y}] = E[\\hat{\\theta}_0] + x. E[\\hat{\\theta}] $$\n",
    "\n",
    "$$\\\\ $$\n",
    "\n",
    "$$ \\textbf{Bias} = \\text{True value} - \\text{Estimated Value} $$\n",
    "\n",
    "$$ E[(y- E[\\hat{y}])^2] = E[(\\text{bias})^2] $$\n",
    "\n",
    "$$ E[(E[\\hat{y}] - \\hat{y})^2] = \\text{variance of predicted values from different model} $$\n",
    "\n",
    "$$ \\textbf{MSE}_{\\text{test}} = E[(\\text{bias})^2] + \\text{variance} $$\n",
    "\n",
    "$$ \\text{We know: }\\ \\ \\text{y} = \\text{f(x)} + \\epsilon $$\n",
    "\n",
    "$$ \\text{Take this } \\epsilon \\text{ out of the bias term } E[(y- E[\\hat{y}])^2] \\text{ leading to :}$$\n",
    "\n",
    "$$\\textbf{MSE}_{\\text{test}} = E[(\\text{bias})^2] + \\text{variance} + \\epsilon $$\n",
    "\n",
    "$$\\text{where, } \\epsilon \\text{ is the irreducible error.}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now** to minimize error in prediction on testing data, we must minimize *bias* and *variance*.\n",
    "\n",
    "**CAUTION** : Bias and variance are complimentary, i.e.  \n",
    "\n",
    "if bias $\\uparrow$ then variance $\\downarrow$  \n",
    "if bias $\\downarrow$ then variance $\\uparrow$ \n",
    "\n",
    "This phenomenon is also called bias variance trade-off. And one must choose such $\\text{f(x)}$ that both bias and variance are low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will understand Linear Discriminant Analysis for Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our data:** For now let's say we have N examples, 1 feature and 2 class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then we know,** $$\\text{According to Naive Baye's} $$ \n",
    "$$\\ $$\n",
    "\n",
    "$$\\textbf{P(C=1|X}_i\\textbf{)} = \\frac{\\text{P(X}_i\\text{|C=1)} \\text{.P(C=1)}}{\\displaystyle \\sum^{1}_{k=0} \\text{P(X}_i\\text{|C=k)} \\text{.P(C=k)}} $$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\frac{\\textbf{P(C=1|X}_i\\textbf{)}}{\\textbf{P(C=0|X}_i\\textbf{)}} = \\frac{\\text{P(X}_i\\text{|C=1)}}{\\text{P(X}_i\\text{|C=0)}}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{Because we are considering that our data is balanced, hence prior prob. cancel out each other}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{Since we have one feature, }\\text{P(X}_i\\text{|C=1)} \\text{ will be:}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{P(X}_i\\text{|C=1)} = \\frac{1}{\\sqrt{2\\hat{\\sigma_1}}} e^{\\frac{-1}{2} \\frac{\\left(X_i - \\hat{\\mu_1}\\right)^2}{2 \\hat{\\sigma}^2_1}}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\frac{\\textbf{P(C=1|X}_i\\textbf{)}}{\\textbf{P(C=0|X}_i\\textbf{)}} = \\frac{\\frac{1}{\\sqrt{2\\hat{\\sigma_1}}} e^{\\frac{-1}{2} \\frac{\\left(X_i - \\hat{\\mu_1}\\right)^2}{ \\hat{\\sigma}^2_1}}} {\\frac{1}{\\sqrt{2\\hat{\\sigma_0}}} e^{\\frac{-1}{2} \\frac{\\left(X_i - \\hat{\\mu_0}\\right)^2}{\\hat{\\sigma}^2_0}}} $$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\textbf{Weak Assumption:}\\ \\ \\ \\ \\hat{\\sigma}_0 = \\hat{\\sigma}_1 = \\hat{\\sigma} $$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\frac{\\textbf{P(C=1|X}_i\\textbf{)}}{\\textbf{P(C=0|X}_i\\textbf{)}} = e^{\\frac{\\left(X_i - \\hat{\\mu_0}\\right)^2 - \\left(X_i - \\hat{\\mu_1}\\right)^2 }{2 \\hat{\\sigma}^2}}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{Solving Further:}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\frac{\\textbf{P(C=1|X}_i\\textbf{)}}{\\textbf{P(C=0|X}_i\\textbf{)}} = e^{\\theta_0 + X_i \\theta_1}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\log_e \\left( {\\frac{\\textbf{P(C=1|X}_i\\textbf{)}}{\\textbf{P(C=0|X}_i\\textbf{)}}} \\right) = \\theta_0 + X_i \\theta_1 $$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{Where, } \\theta_0 + X_i \\theta_1 \\text{ is called }\\textbf{Discriminant Score} \\text{.}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{And, }\\ \\ \\ \\theta_0 = - \\frac{\\left(\\hat{\\mu}^2_0- \\hat{\\mu}^2_1\\right)}{2 \\hat{\\sigma}^2}\\ \\ \\ \\ \\theta_1 = \\frac{\\left(\\hat{\\mu}_0 - \\hat{\\mu}_1 \\right)}{\\hat{\\sigma}^2}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\textbf{Case:} \\text{ when } \\textbf{P(C=1|X}_i\\textbf{)} \\text{ and } \\textbf{P(C=0|X}_i\\textbf{)} \\text{ are equal} \\Rightarrow $$\n",
    "$$\\ $$\n",
    "\n",
    "Our algorithm will get confused in such case scenario. For that value of $X_i$ for which our algorithm cannot decide which class to choose that value is called **Threshold Point**. And the value of $\\textbf{P(C=1|X}_i\\textbf{)}$ at that $X_i$ is called **Threshold Value**.\n",
    "$$\\ $$\n",
    "\n",
    "**Let's find that threshold value:**\n",
    "\n",
    "$$ \\theta_0 + X_i \\theta_1 = 0 $$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{putting the values of thetas, we get } X_i = \\frac{\\hat{\\mu}_1 + \\hat{\\mu}_0} {2} $$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{Now,}\\ \\ \\ \\ \\frac{\\textbf{P(C=1|X}_i\\textbf{)}}{\\textbf{P(C=0|X}_i\\textbf{)}} = \\frac{\\textbf{P(C=1|X}_i\\textbf{)}}{1 - \\textbf{P(C=1|X}_i\\textbf{)}} = e^{\\theta_0 + X_i \\theta_1} $$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{And take } \\theta_0 = -\\theta_0 \\text{ ; } \\theta_1 = -\\theta_1 $$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\textbf{P(C=1|X}_i\\textbf{)} = \\frac{1}{1 + e^{- \\left(\\hat{\\theta}_0 + X_i \\hat{\\theta}_1 \\right)}}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\textbf{Hence: } \\textbf{P(C=1|X}_i\\textbf{)} \\text{ is } \\textbf{Sigmoid} \\text{,} \\\\ \\text{while, } \\text{P(X}_i\\text{|C=1)} \\text{ is } \\text{Gaussian} \\text{.}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{Also, putting } X_i = \\frac{\\hat{\\mu}_1 + \\hat{\\mu}_0} {2} \\text{ in } \\textbf{P(C=1|X}_i\\textbf{)} \\text{, we get } \\textbf{P(C=1|X}_i\\textbf{)} = \\textbf{0.5} $$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{Hence 0.5 is our threshold value}$$\n",
    "$$\\ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When $\\hat{\\sigma^2_1} = \\hat{\\sigma^2_0} = \\hat{\\sigma^2}$ cannot hold true:**\n",
    "\n",
    "We assume then $\\hat{\\sigma^2_1} = \\hat{\\sigma^2_0} = \\hat{\\sigma^2_p}$, the pooled variance as $\\hat{\\sigma^2_p} = \\frac{N_1 \\hat{\\sigma^2_1} + N_0 \\hat{\\sigma^2_0}}{N_1+N_0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we have more that one features:** Let's say 2 features -\n",
    "\n",
    "$$\\text{Discriminant Score will look like:}\\ \\ \\ \\ \\hat{\\theta_0} + x_1 \\hat{\\theta_1} + x_2 \\hat{\\theta_2}$$\n",
    "\n",
    "$$\\text{which is equation of a plane.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When $\\Sigma_0 \\neq \\Sigma_1 $:** \n",
    "\n",
    "**Solution: QDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to find threshold value for multiclass?** \n",
    "\n",
    "Explained in Multi-class Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

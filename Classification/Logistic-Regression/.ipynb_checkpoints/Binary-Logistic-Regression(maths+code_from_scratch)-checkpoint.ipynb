{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Binary Logistic Regression** using Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Logistic Regression** is a discriminative classifier, because here, instead of first calculating likelihood probability and the calculating posterior probability, we will directly calculate posterior probability by making a PDF for posterior probability. \n",
    "### Let's see how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know, that our posterior probaility, for each example in our data(rows), for one of the class(if our data has only 2 classes) looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{P(class='1' | X)} = \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, for another class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{P(class='0' | X)} = 1 - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose :** our data(considering preprocessed and normalized data) has N rows(examples) and M columns(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** For each example i in all N examples, there will be actual class that it must belong to. That class will be either `0` or `1` (in encoded form)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Let's see how class column will look like: \n",
    "\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "1\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Now what we are going to make PDF of posterior probability is that we are going to combine posterior probability of each class in a **likelihood function**. Let's see how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A.*** For each example i, posterior probaility can be written as :\n",
    "    \n",
    "$$\\boldsymbol{ \\textbf{P(class=}C_i\\textbf{)}} = (\\hat p)^{C_i}\\ (1-\\hat p)^{1 - C_i}$$ \n",
    "\n",
    "where $\\hat p$ is the posterior probability for class `1` and $C_i$ is the class label for a given example $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***B.*** Combining above formula for all examples :\n",
    "\n",
    "$\\textbf{Likelihood probability} $ for $(x_1 = \\text{'0'}\\ \\cap\\ x_2 = \\text{'0'}\\ \\cap\\ \\dots \\cap\\ x_N = \\text{'1'})\\ $ is:\n",
    "\n",
    "\n",
    "$$\\textbf{L}\\ =\\ \\prod^N_{i = 1}\\ (\\hat p)^{C_i}\\ (1- \\hat p)^{1 - C_i}$$\n",
    "\n",
    "\n",
    "$$\\textbf{where,}\\ \\ \\ \\ \\ {\\hat p}\\ =\\ \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}$$ \n",
    "\n",
    "\n",
    "$$\\textbf{and}\\ C_i \\text{is class label for}\\ i^{\\text{th}} example$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***C.*** Taking both side $\\log_e$, our Likelihood Function becomes:\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}}\\ =\\ \\sum_{i=1}^{N}\\ \\left[C_i\\ .\\log_e{\\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}}\\right]\\ \\ +\\  \\left[(1 - C_i)\\ .\\log_e{\\left(1 - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)}\\right]$$\n",
    "\n",
    "$$\\ $$   \n",
    "\n",
    "This is our final **Log Likelihood Function**, which we have to maximize, i.e:\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\underset{\\hat\\theta_0,\\hat\\theta_1, \\hat\\theta_2}{\\textbf{max}}}\\ \\ \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}$$\n",
    "\n",
    "$$\\textbf{=>}\\ \\ \\ \\text{-}\\ \\boldsymbol{\\underset{\\hat\\theta_0,\\hat\\theta_1, \\hat\\theta_2}{\\textbf{min}}}\\ \\ \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}$$\n",
    "\n",
    "$$\\ $$\n",
    "To dissolve the negative sign outside our optimization problem, we take in inside our log likelihood function and the our new likelihood function and optimization problem becomes :\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{- \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}}\\ =\\ - \\ \\sum_{i=1}^{N}\\ \\left[C_i\\ .\\log_e{\\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}}\\right]\\ \\ +\\ \\ \\left[(1 - C_i)\\ .\\log_e{1 - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}}\\right]$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\underset{\\hat\\theta_0,\\hat\\theta_1, \\hat\\theta_2}{\\textbf{min}}}\\ \\ - \\ \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Now we are going to solve our minimization problem using **Gradient Descent**:\n",
    "$$\\ $$\n",
    "\n",
    "$$\\theta_{\\textbf{0}, final}\\ =\\ \\theta_{\\textbf{0}, initial} + \\epsilon\\ .\\frac{\\partial}{\\partial\\theta_{\\textbf{0}}}\\left[\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}\\right] \\Bigg|_{\\theta_0 = \\theta_{0,initial}\\\\ \\theta_1 = \\theta_{1,initial}\\\\ \\theta_2 = \\theta_{2, initial}}$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\theta_{\\text{final}}}\\ = \\ \\begin{bmatrix}\n",
    "                                                \\theta_{\\textbf{1},final} \\\\\n",
    "                                                \\theta_{\\textbf{2},final}\n",
    "                                                \\end{bmatrix}\n",
    "                                                \\ =\\ \\begin{bmatrix}\n",
    "                                                \\theta_{\\textbf{1},initial} \\\\\n",
    "                                                \\theta_{\\textbf{2},initial}\n",
    "                                                \\end{bmatrix} \n",
    "                                                \\ +\\ \\epsilon\\ .\\nabla\\log_e{L(\\theta_0, \\theta_1, \\theta_2) \\Bigg|_{\\theta_0 = \\theta_{0,initial}\\\\ \\theta_1 = \\theta_{1,initial}\\\\ \\theta_2 = \\theta_{2, initial}}}\n",
    "                                                $$\n",
    "                                                \n",
    "                                                \n",
    "$$\\textbf{where,}\\ \\ \\ \\boldsymbol{\\epsilon}\\ \\text{is the step-size}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A.*** Calculating $\\boldsymbol{\\frac{\\partial}{\\partial\\theta_{\\textbf{0}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}$ and $\\boldsymbol{\\nabla\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}$ we find their values as:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial}{\\partial\\theta_{\\textbf{0}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}\\ =\\ -\\ \\sum_{i=0}^{N}\\left(C_i - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\nabla\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}\\ = \\ \\begin{bmatrix}\n",
    "                            \\frac{\\partial}{\\partial\\theta_{\\textbf{1}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)} \\\\\n",
    "                            \\frac{\\partial}{\\partial\\theta_{\\textbf{2}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}\n",
    "                            \\end{bmatrix}\\ = \\ \n",
    "                            \\begin{bmatrix}\n",
    "                 -\\ \\sum\\limits_{i=1}^{N}\\ x_1^i\\ .\\left(C_i - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)\\\\\n",
    "                 -\\ \\sum\\limits_{i=1}^{N}\\ x_2^i\\ .\\left(C_i - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)\n",
    "                 \\end{bmatrix}\n",
    "                            $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Code It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ini = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_ini.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels=['Unnamed: 32', 'id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'].replace(to_replace=['M', 'B'], value=[1,0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.iloc[:int(0.75*data.shape[0]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = data.iloc[int(0.75*data.shape[0]):, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = training_data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array(C).reshape(C.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data.drop(labels='diagnosis', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean())-X.std()  #Normalizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "\n",
    "m = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_theta0_theta(theta0,theta,X):\n",
    "    \n",
    "    D = theta0 + np.matmul(X,theta)\n",
    "    \n",
    "    E = np.exp(-D)\n",
    "    \n",
    "    H = 1/(1+E)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_loss(C,H):\n",
    "    \n",
    "    C_logH = np.matmul(C.T,np.log(H))\n",
    "    \n",
    "    one_C_1_H = np.matmul((1-C).T,np.log(1-H))\n",
    "    \n",
    "    negative_log_loss = (-1/N)*(C_logH + one_C_1_H)\n",
    "    \n",
    "    return negative_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivatives(X,H,C):\n",
    "    \n",
    "    del_theta0 = np.mean(H-C)\n",
    "    \n",
    "    del_theta = (1/N)*(np.matmul((H-C).T,X)).T\n",
    "    \n",
    "    return del_theta0, del_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 10**(-7)\n",
    "\n",
    "alpha = 10**(-5)\n",
    "\n",
    "theta0_initial = 0\n",
    "\n",
    "theta_initial = np.zeros((m,1))\n",
    "\n",
    "iterations = []\n",
    "\n",
    "neg_log_loss_history = []\n",
    "\n",
    "iteration_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while (True):\n",
    "    \n",
    "    H_theta0_theta_initial = H_theta0_theta(theta0_initial,theta_initial,X)\n",
    "    del_theta0_initial,del_theta_initial = derivatives(X,H_theta0_theta_initial,C)\n",
    "    \n",
    "    \n",
    "    theta0_final = theta0_initial - (alpha * del_theta0_initial)\n",
    "    theta_final = theta_initial - (alpha * del_theta_initial)\n",
    "    \n",
    "    \n",
    "    H_theta0_theta_final = H_theta0_theta(theta0_final,theta_final,X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    neg_log_loss_initial = neg_log_loss(C,H_theta0_theta_initial)\n",
    "    neg_log_loss_final = neg_log_loss(C,H_theta0_theta_final)\n",
    "    \n",
    "    \n",
    "    if abs(neg_log_loss_final - neg_log_loss_initial) < epsilon:\n",
    "        break\n",
    "        \n",
    "    theta0_initial = theta0_final\n",
    "    theta_initial = theta_final\n",
    "    \n",
    "    iterations.append(iteration_number)\n",
    "    neg_log_loss_history.append(neg_log_loss_initial)\n",
    "    \n",
    "    \n",
    "    print(\"Iteration number =\",iteration_number,\"Neg log loss=\",neg_log_loss_initial)\n",
    "    iteration_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels = testing_data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels = np.array(testing_labels).reshape(testing_labels.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testing_data.drop(['diagnosis'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test/np.mean(X_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_probability_malignant = H_theta0_theta(theta0_final,theta_final,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.uint8(posterior_probability_malignant > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = np.sum(np.equal(predicted_classes,testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(correct_count/X_test.shape[0])*100    # accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

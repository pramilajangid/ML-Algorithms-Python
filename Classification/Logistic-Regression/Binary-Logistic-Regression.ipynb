{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Binary Logistic Regression** using Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Logistic Regression** is a discriminative classifier, because here, instead of first calculating likelihood probability and the calculating posterior probability, we will directly calculate posterior probability by making a PDF for posterior probability. \n",
    "### Let's see how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know, that our posterior probaility, for each example in our data(rows), for one of the class(if our data has only 2 classes) looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{P(class='1' | X)} = \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, for another class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{P(class='0' | X)} = 1 - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose :** our data(considering preprocessed and normalized data) has N rows(examples) and M columns(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** For each example i in all N examples, there will be actual class that it must belong to. That class will be either `0` or `1` (in encoded form)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Let's see how class column will look like: \n",
    "\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "1\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Now what we are going to make PDF of posterior probability is that we are going to combine posterior probability of each class in a **likelihood function**. Let's see how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A.*** For each example i, posterior probaility can be written as :\n",
    "    \n",
    "$$\\boldsymbol{ \\textbf{P(class=}C_i\\textbf{)}} = (\\hat p)^{C_i}\\ (1-\\hat p)^{1 - C_i}$$ \n",
    "\n",
    "where $\\hat p$ is the posterior probability for class `1` and $C_i$ is the class label for a given example $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***B.*** Combining above formula for all examples :\n",
    "\n",
    "$\\textbf{Likelihood probability} $ for $(x_1 = \\text{'0'}\\ \\cap\\ x_2 = \\text{'0'}\\ \\cap\\ \\dots \\cap\\ x_N = \\text{'1'})\\ $ is:\n",
    "\n",
    "\n",
    "$$\\textbf{L}\\ =\\ \\prod^N_{i = 1}\\ (\\hat p)^{C_i}\\ (1- \\hat p)^{1 - C_i}$$\n",
    "\n",
    "\n",
    "$$\\textbf{where,}\\ \\ \\ \\ \\ {\\hat p}\\ =\\ \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}$$ \n",
    "\n",
    "\n",
    "$$\\textbf{and}\\ C_i \\text{is class label for}\\ i^{\\text{th}} example$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***C.*** Taking both side $\\log_e$, our Likelihood Function becomes:\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}}\\ =\\ \\sum_{i=1}^{N}\\ \\left[C_i\\ .\\log_e{\\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}}\\right]\\ \\ +\\  \\left[(1 - C_i)\\ .\\log_e{\\left(1 - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)}\\right]$$\n",
    "\n",
    "$$\\ $$   \n",
    "\n",
    "This is our final **Log Likelihood Function**, which we have to maximize, i.e:\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\underset{\\hat\\theta_0,\\hat\\theta_1, \\hat\\theta_2}{\\textbf{max}}}\\ \\ \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}$$\n",
    "\n",
    "$$\\textbf{=>}\\ \\ \\ \\text{-}\\ \\boldsymbol{\\underset{\\hat\\theta_0,\\hat\\theta_1, \\hat\\theta_2}{\\textbf{min}}}\\ \\ \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}$$\n",
    "\n",
    "$$\\ $$\n",
    "To dissolve the negative sign outside our optimization problem, we take in inside our log likelihood function and the our new likelihood function and optimization problem becomes :\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{- \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}}\\ =\\ - \\ \\sum_{i=1}^{N}\\ \\left[C_i\\ .\\log_e{\\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}}\\right]\\ \\ \\left[(1 - C_i)\\ .\\log_e{1 - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}}\\right]$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\underset{\\hat\\theta_0,\\hat\\theta_1, \\hat\\theta_2}{\\textbf{min}}}\\ \\ - \\ \\log_e{\\left(\\textbf{L}(\\theta_0, \\theta_1, \\theta_2)\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Now we are going to solve our minimization problem using **Gradient Descent**:\n",
    "$$\\ $$\n",
    "\n",
    "$$\\theta_{\\textbf{0}, final}\\ =\\ \\theta_{\\textbf{0}, initial} + \\epsilon\\ .\\frac{\\partial}{\\partial\\theta_{\\textbf{0}}}\\left[\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}\\right] \\Bigg|_{\\theta_0 = \\theta_{0,initial}\\\\ \\theta_1 = \\theta_{1,initial}\\\\ \\theta_2 = \\theta_{2, initial}}$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\theta_{\\text{final}}}\\ = \\ \\begin{bmatrix}\n",
    "                                                \\theta_{\\textbf{1},final} \\\\\n",
    "                                                \\theta_{\\textbf{2},final}\n",
    "                                                \\end{bmatrix}\n",
    "                                                \\ =\\ \\begin{bmatrix}\n",
    "                                                \\theta_{\\textbf{1},initial} \\\\\n",
    "                                                \\theta_{\\textbf{2},initial}\n",
    "                                                \\end{bmatrix} \n",
    "                                                \\ +\\ \\epsilon\\ .\\nabla\\log_e{L(\\theta_0, \\theta_1, \\theta_2) \\Bigg|_{\\theta_0 = \\theta_{0,initial}\\\\ \\theta_1 = \\theta_{1,initial}\\\\ \\theta_2 = \\theta_{2, initial}}}\n",
    "                                                $$\n",
    "                                                \n",
    "                                                \n",
    "$$\\textbf{where,}\\ \\ \\ \\boldsymbol{\\epsilon}\\ \\text{is the step-size}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A.*** Calculating $\\boldsymbol{\\frac{\\partial}{\\partial\\theta_{\\textbf{0}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}$ and $\\boldsymbol{\\nabla\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}$ we find their values as:\n",
    "\n",
    "$$\\boldsymbol{\\frac{\\partial}{\\partial\\theta_{\\textbf{0}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}\\ =\\ -\\ \\sum_{i=0}^{N}\\left(C_i - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\boldsymbol{\\nabla\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}}\\ = \\ \\begin{bmatrix}\n",
    "                            \\frac{\\partial}{\\partial\\theta_{\\textbf{1}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)} \\\\\n",
    "                            \\frac{\\partial}{\\partial\\theta_{\\textbf{2}}}\\log_e{L(\\theta_0, \\theta_1, \\theta_2)}\n",
    "                            \\end{bmatrix}\\ = \\ \n",
    "                            \\begin{bmatrix}\n",
    "                 -\\ \\sum\\limits_{i=1}^{N}\\ x_1^i\\ .\\left(C_i - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)\\\\\n",
    "                 -\\ \\sum\\limits_{i=1}^{N}\\ x_2^i\\ .\\left(C_i - \\frac{1}{1 + e^{-(\\hat\\theta_0 + \\hat\\theta^{T}.X)}}\\right)\n",
    "                 \\end{bmatrix}\n",
    "                            $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Code It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ini = pd.read_csv('/home/pramila/Desktop/DataSets/TumorData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_ini.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels=['Unnamed: 32', 'id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'].replace(to_replace=['M', 'B'], value=[1,0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.iloc[:int(0.75*data.shape[0]),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = data.iloc[int(0.75*data.shape[0]), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = training_data['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array(C).reshape(C.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data.drop(labels='diagnosis', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean())-X.std()  #Normalizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_sigmoid_output(theta0, theta, X):\n",
    "    '''calculates posterior probability, i.e probability represented as sigmoid function\n",
    "    '''\n",
    "    exp_power = theta0 + (np.matmul(theta.T, X))\n",
    "    \n",
    "    denominator = 1 + np.exp(-exp_power)\n",
    "    \n",
    "    return 1/denominator   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(theta0, theta, X, C):\n",
    "    '''Calculates log likelihood function's value for given thetas\n",
    "    '''\n",
    "    sigmoid_output = the_sigmoid_output(theta0, theta, X)\n",
    "    \n",
    "    first_term = np.matmul(C.T, np.log(sigmoid_output))\n",
    "    \n",
    "    second_term = np.matmul((1-C).T, np.log(1 - sigmoid_output))\n",
    "        \n",
    "    return first_term + second_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_theta0(theta0, theta, X):\n",
    "    '''Returns derivative with respect to theta0\n",
    "    '''\n",
    "    sigmoid_output = the_sigmoid_output(theta0, theta, X)\n",
    "    \n",
    "    return C - sigmoid_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_theta(theta0, theta, X):\n",
    "    '''Returns derivative with respect to theta \n",
    "    '''\n",
    "    sigmoid_output = the_sigmoid_output(theta0, theta, X)\n",
    "    \n",
    "    return np.matmul(X.T, (C - sigmoid_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 10**(-7)\n",
    "\n",
    "step_size = 10**(-4)\n",
    "\n",
    "theta0_initial = \n",
    "\n",
    "theta_initial = \n",
    "\n",
    "while True:\n",
    "    \n",
    "    derivative0 = derivative_theta0(theta0_initial, theta_initial, X)\n",
    "    \n",
    "    gradient_matrix = derivative_theta(theta0_initial, theta_initial, X)\n",
    "    \n",
    "    theta0_final = theta0_initial + (step_size * (derivative0))\n",
    "    \n",
    "    theta_final = theta_initial + (step_size * (gradient_matrix))\n",
    "    \n",
    "    if (abs(neg_log_likelihood(theta0_final, theta_final) - neg_log_likelihood(theta0_final, theta_final)) < tolerance) :\n",
    "        break\n",
    "    \n",
    "    theta0_initial =  theta0_final\n",
    "    theta_initial = theta_final\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

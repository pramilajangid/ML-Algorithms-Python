{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Let's try to understand what is going on in our algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We know:** \n",
    "\n",
    "#### Baye's rule for multi-class \n",
    "\n",
    "$$\\textbf{P(C}^i\\textbf{ = 2 | X}^i\\textbf{)} = \\left(\\frac{\\text{P(X}^i\\text{= 2 | C}^i\\text{)}. \\text{P(C}^i\\text{ = 2)}}{\\displaystyle\\sum^{K}_{k=0} \\text{P(X}^i\\text{= k | C}^i\\text{)}. \\text{P(C}^i\\text{ = k)}}\\right) \\ \\ \\ \\ \\cdots \\{1\\}$$\n",
    "\n",
    "**Assumption:** \n",
    "- Our dataset is balanced, if it is not, create artificial data.\n",
    "\n",
    "**Remove:**\n",
    "- Our dataset is balanced hence we don't need to multiply prior probability, it will be redundant for comparison\n",
    "- We can remove denominator from above expression because it is common in all terms and hence redundant for comparison.\n",
    "\n",
    "**{1} becomes:**\n",
    "\n",
    "$$\\textbf{P(C}^i\\textbf{ = 2 | X}^i\\textbf{)} \\approx \\text{P(X}^i\\text{= 2 | C}^i\\text{)}\\ \\ \\ \\ \\cdots \\{2\\}$$\n",
    "\n",
    "**We know:**\n",
    "\n",
    "$x = e^{\\log_{e} x}$\n",
    "\n",
    "**{2} becomes:**\n",
    "\n",
    "$$\\textbf{P(C}^i\\textbf{ = 2 | X}^i\\textbf{)} = \\frac{e^{\\log_{e} \\text{P(X}^i\\text{= 2 | C}^i\\text{)} }}{\\displaystyle \\sum^{K}_{k=0} e^{\\log_{e} \\text{P(X}^i\\text{= k | C}^i\\text{)} }}\\ \\ \\ \\ \\cdots \\{3\\}$$\n",
    "\n",
    "$$\\textbf{P(C}^i\\textbf{ = 2 | X}^i\\textbf{)} \\approx e^{\\log_{e} \\text{P(X}^i\\text{= 2 | C}^i\\text{)} } $$\n",
    "\n",
    "$$\\textbf{Assumption:} \\text{ For now, that our data comes from Gaussian Distribution}$$\n",
    "\n",
    "$$\\text{P(X}^i\\text{= 2 | C}^i\\text{)} = \\frac{1}{{\\sqrt {2 \\pi}}^N \\sqrt{|\\Sigma_2|}} e^\\left({\\frac{-1}{2} \\begin{bmatrix} X^1_i - \\mu^1_2\\\\ \\vdots \\\\ X^N_i - \\mu^N_2 \\end{bmatrix}. \\Sigma^T . {\\begin{bmatrix} X^1_i - \\mu^1_2 \\\\ \\vdots \\\\ X^N_i - \\mu^N_2 \\end{bmatrix}}^T}\\right)$$\n",
    "\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{X}_i \\Rightarrow N\\times i $$ \n",
    "\n",
    "\n",
    "$$\\text{Let's say we have 3 classes, then:}\\\\ \\Sigma_0 = \\Sigma_1 = \\Sigma_2 = \\Sigma_p$$\n",
    "\n",
    "$$\\text{And } \\log_e \\text{P(X}^i\\text{= 2 | C}^i\\text{)} \\text{ becomes:}\\\\ = \\log_e \\frac{1}{{\\sqrt {2 \\pi}}^N \\sqrt{|\\Sigma_p|}} - \\left(\\frac{1}{2} \\begin{bmatrix} X^1_i - \\mu^1_2\\\\ \\vdots \\\\ X^N_i - \\mu^N_2 \\end{bmatrix}. \\Sigma^T . {\\begin{bmatrix} X^1_i - \\mu^1_2 \\\\ \\vdots \\\\ X^N_i - \\mu^N_2 \\end{bmatrix}}^T \\right)$$\n",
    "\n",
    "$$\\text{We can remove } \\frac{1}{{\\sqrt {2 \\pi}}^N \\sqrt{|\\Sigma_p|}} \\text{ because it will be constant in numerator and denominator in } \\textbf{P(C}^i\\textbf{ = 2 | X}^i\\textbf{)}$$\n",
    "\n",
    "$$\\text{Now, it can be observed that our numerator and denominators in } \\textbf{P(C}^i\\textbf{ = 2 | X}^i\\textbf{)} \\text{ are of the form: }\\\\\n",
    "\\\\\n",
    "\\log_{e} \\text{P(X}^i\\text{= 2 | C}^i\\text{)} = \\hat{\\theta}^2_0 + \\hat{\\theta}^{2 T}.X_i\n",
    "\\\\\n",
    "\\log_{e} \\text{P(X}^i\\text{= 0 | C}^i\\text{)} = \\hat{\\theta}^0_0 + \\hat{\\theta}^{0 T}.X_i\n",
    "\\\\\n",
    "\\log_{e} \\text{P(X}^i\\text{= 1 | C}^i\\text{)} = \\hat{\\theta}^1_0 + \\hat{\\theta}^{1 T}.X_i$$\n",
    "$$\\ $$\n",
    "\n",
    "$$ \\text{Hence {3} becomes }$$\n",
    "$$\\ $$\n",
    "$$\\boxed{\\textbf{P(C}^i\\textbf{ = 2 | X}^i\\textbf{)} = \\frac{e^{\\hat{\\theta}^2_0 + \\hat{\\theta}^{2 T}.X_i}} {\\displaystyle \\sum^{K}_{k=0} e^{\\hat{\\theta}^k_0 + \\hat{\\theta}^{k T}.X_i}}}$$\n",
    "\n",
    "$$\\text{This is called } \\textbf{Softmax Function}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation of log loss for Multiclass LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PDF for multinomial experiment can be formulated as:**\n",
    "\n",
    "Suppose we our 3 classes(1,2,3) and in our data, class labels for each example are as:\n",
    "\n",
    "$$\\begin{matrix}\\text{class} \\\\ 3 \\\\ 2 \\\\ 1 \\\\ 3 \\\\ 1 \\\\ \\vdots \\\\ 2 \\end{matrix}$$\n",
    "\n",
    "$$\\text{Each class tag can be encoded via Onehot Encoder as:}$$\n",
    "\n",
    "$$\\begin{matrix}\\text{class} \\\\C_1 C_2 C_3 \\\\ 1\\ 0\\ 0 \\\\ 0\\ 1\\ 0 \\\\ 0\\ 0\\ 1 \\\\ 1\\ 0\\ 0 \\\\ 0\\ 1\\ 0 \\\\ \\vdots \\\\ 0\\ 0\\ 1 \\end{matrix}$$\n",
    "$$\\ $$\n",
    "\n",
    "$$\\text{Hence PDF can be formulated as: }$$\n",
    "$$\\ $$\n",
    "$$\\textbf{P(C}^i\\textbf{ = 2 | X}^i\\textbf{)} = \\left[\\text{P(C}^i\\text{ = 1 | X}^i\\text{)} \\right]^{C^i_1} . \\left[\\text{P(C}^i\\text{ = 2 | X}^i\\text{)} \\right]^{C^i_2} . \\left[\\text{P(C}^i\\text{ = 3 | X}^i\\text{)} \\right]^{C^i_3} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now likelihood function for all the examples becomes:\n",
    "\n",
    "$$L(\\theta^1_0, \\theta^1, \\theta^2_0, \\theta^2, \\theta^3_0, \\theta^3) = \\prod^N_{m=1} P(C^m|X_m)$$\n",
    "\n",
    "$$ = \\prod^{N}_{m=1} \\left ( \\left[\\text{P(C}^m\\text{ = 1 | X}^m\\text{)} \\right]^{C^m_1} . \\left[\\text{P(C}^m\\text{ = 2 | X}^m\\text{)} \\right]^{C^m_2} . \\left[\\text{P(C}^m\\text{ = 3 | X}^m\\text{)} \\right]^{C^m_3} \\right)$$\n",
    "\n",
    "$$= \\prod^{N}_{m=1} \\prod^{K}_{k=1} \\left(\\left[\\text{P(C}^m\\text{ = k | X}^m\\text{)} \\right]^{C^m_k} \\right)$$\n",
    "\n",
    "**Log Likelihood Function :**\n",
    "\n",
    "$$\\log_e {L(\\theta^1_0, \\theta^1, \\theta^2_0, \\theta^2, \\theta^3_0, \\theta^3)} = \\sum^{N}_{m=1} \\sum^{K}_{k=1} C^m_k . \\text{P(C}^m\\text{ = k | X}^m\\text{)} $$\n",
    "\n",
    "**Our optimization problem:**\n",
    "\n",
    "We need to maximize $\\log_e L(\\theta^1_0, \\theta^1, \\theta^2_0, \\theta^2, \\theta^3_0, \\theta^3)$. In other words, find such thetas such that $\\log_e L(\\theta^1_0, \\theta^1, \\theta^2_0, \\theta^2, \\theta^3_0, \\theta^3)$ is maximum. Or we can say, minimize $- \\log_e L(\\theta^1_0, \\theta^1, \\theta^2_0, \\theta^2, \\theta^3_0, \\theta^3)$ for all the thetas. \n",
    "\n",
    "This can be achieved via Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's code it  \n",
    "We will code multi-class logistic regression with Devnagri Handwritten character dataset, in seprerate notebook, within same folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
